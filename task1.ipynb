{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Few Important Point**\n",
    "As i did not have an openai api key. So i had to use the free tier of the groq api. So now all the things that I will do in the next code will be change from how I was to do with OpenAi api. They give much more options, like they provide even 32k token limit, which is much more that groq gives which is 6k. Also there is a limit on how much requests I can make in a second. So I will have to make sure that I do not make too many requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY=os.getenv('GROQ_API_KEY')\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for loading the document I got help from https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/\n",
    "I will be using code example they have given into my code and usecase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"crime-and-punishment.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how many pages we have right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767\n"
     ]
    }
   ],
   "source": [
    "print(len(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to create my own document object from the pages text , I used the code this link. https://python.langchain.com/docs/how_to/document_loader_custom/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "combined_text = \" \".join(page.page_content for page in pages)\n",
    "\n",
    "combined_document = Document(page_content=combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this link I got the code for recursive text splitter. https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n",
    "As i need smaller chunks so that my model can process it, without any token limit error. As the model limit is 6000 so I am choosing the chunk size by hit and trial. And after checking which chunk size is perfect. Also I am overlapping size is 500, which helps in getting the context of the of previus chunk.\n",
    "Learned about the text spliiter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=22700, chunk_overlap=500)\n",
    "splits = text_splitter.split_documents([combined_document])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how many chunks we have to process here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Summarize only the unique events and details from the text, capturing essential plot points and themes without repeating introductory statements or prior events. Avoid starting with the introductory phrases like 'Here is a summary' and keep each summary segment cohesive and streamlined. Don't create any bullet point rather right summary in paragraphs.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are generating a batch summary of a section of the book, based on multiple summary segments provided. Please condense these segments into a coherent and concise summary that captures the main points, themes, and character developments of this batch. Avoid repeating details from individual segments and focus on providing an accurate overview of this portion of the narrative.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the current approach here is first generate the summary for chunks and then make batches of 8, and summarize them to generate the final summary. This is a bit of a hack, but it seems to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = \"\"\n",
    "summaries = \"\"\n",
    "i=0\n",
    "for i, split in enumerate(splits):\n",
    "    # Generate summary for each chunk\n",
    "    chain = prompt | llm\n",
    "    summary = chain.invoke({\"messages\": [HumanMessage(content=split.page_content)]})\n",
    "    summaries+=summary.content\n",
    "    print(summary)\n",
    "    # Every 8 summaries, create a batch summary\n",
    "    if (i + 1) % 8 == 0 or (i + 1) == len(splits):  # Ensures last batch is also summarized\n",
    "        # Summarize the combined text from this batch\n",
    "        batch_chain = batch_prompt | llm\n",
    "        batch_summary = batch_chain.invoke({\"messages\": [HumanMessage(content=batch_text)]})\n",
    "        \n",
    "        # Add batch summary to final summary\n",
    "        final_summary += batch_summary.content + \"\\n\\n\"\n",
    "        \n",
    "        # Reset summaries list for next batch\n",
    "        summaries = \"\"\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now inorder to generate a pdf file, we need to use a library that can handle pdf generation. One such library is ```fpdf``` So i will be using it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = FPDF()\n",
    "pdf.set_auto_page_break(auto=True, margin=15)\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I faced this error\n",
    "``` UnicodeEncodeError: 'latin-1' codec can't encode characters in position 824-825: ordinal not in range(256)```\n",
    "\n",
    "Solution was found here ```https://stackoverflow.com/questions/67130517/fpdf-unicodeencodeerror-latin-1-codec-cant-encode-character-u2013-in-pos```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_text = final_summary.replace(\"\\n\", \" \").encode('utf-8').decode('latin-1')\n",
    "pdf.multi_cell(0, 7, txt=formatted_text + \"\\n\\n\")  \n",
    "    \n",
    "pdf_file = \"summarized_crime_and_punishment.pdf\"\n",
    "pdf.output(pdf_file)    \n",
    "\n",
    "print(f\"Summarized PDF saved as {pdf_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Import note***\n",
    "I could not generate full video as i was constantly getting an error.\n",
    "```  raise self._make_status_error_from_response(err.response) from None\n",
    "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j084czcke8m836230scy1s61` on : Limit 500000, Used 498368, Requested 5843. Please try again in 12m7.5494s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
    "(/home/shirjeel/Personal/Assignment/venv) shirjeel@shirjeel:~/Personal/Assignment$ python app.py \n",
    "53```\n",
    "\n",
    "Which simply means that i need a paid account which i dont have for now, so what i was able to do is to generate for few chunks. I will attach this pdf. It shows promising results. Which also shows that given a paid api. It will generate full summary of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
