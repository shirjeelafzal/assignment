{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Few Important Point**\n",
    "As i did not have an openai api key. So i had to use the free tier of the groq api. So now all the things that I will do in the next code will be change from how I was to do with OpenAi api. They give much more options, like they provide even 32k token limit, which is much more that groq gives which is 6k. Also there is a limit on how much requests I can make in a second. So I will have to make sure that I do not make too many requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY=os.getenv('GROQ_API_KEY')\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for loading the document I got help from https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/\n",
    "I will be using code example they have given into my code and usecase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"crime-and-punishment.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how many pages we have right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767\n"
     ]
    }
   ],
   "source": [
    "print(len(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to create my own document object from the pages text , I used the code this link. https://python.langchain.com/docs/how_to/document_loader_custom/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "combined_text = \" \".join(page.page_content for page in pages)\n",
    "\n",
    "combined_document = Document(page_content=combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this link I got the code for recursive text splitter. https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n",
    "As i need smaller chunks so that my model can process it, without any token limit error. As the model limit is 6000 so I am choosing the chunk size by hit and trial. And after checking which chunk size is perfect. Also I am overlapping size is 500, which helps in getting the context of the of previus chunk.\n",
    "Learned about the text spliiter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=22700, chunk_overlap=500)\n",
    "splits = text_splitter.split_documents([combined_document])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how many chunks we have to process here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Summarize only the unique events and details from the text, capturing essential plot points and themes without repeating introductory statements or prior events. Avoid starting with the introductory phrases like 'Here is a summary' and keep each summary segment cohesive and streamlined. Don't create any bullet point rather right summary in paragraphs.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are generating a batch summary of a section of the book, based on multiple summary segments provided. Please condense these segments into a coherent and concise summary that captures the main points, themes, and character developments of this batch. Avoid repeating details from individual segments and focus on providing an accurate overview of this portion of the narrative.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the current approach here is first generate the summary for chunks and then make batches of 8, and summarize them to generate the final summary. This is a bit of a hack, but it seems to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = \"\"\n",
    "summaries = []\n",
    "i=0\n",
    "for i, split in enumerate(splits):\n",
    "    # Generate summary for each chunk\n",
    "    chain = prompt | llm\n",
    "    summary = chain.invoke({\"messages\": [HumanMessage(content=split.page_content)]})\n",
    "    summaries.append(summary.content)\n",
    "    print(summary)\n",
    "    # Every 8 summaries, create a batch summary\n",
    "    if (i + 1) % 8 == 0 or (i + 1) == len(splits):  # Ensures last batch is also summarized\n",
    "        # Combine batch summaries\n",
    "        batch_text = \" \".join(summaries)\n",
    "        \n",
    "        # Summarize the combined text from this batch\n",
    "        batch_chain = batch_prompt | llm\n",
    "        batch_summary = batch_chain.invoke({\"messages\": [HumanMessage(content=batch_text)]})\n",
    "        \n",
    "        # Add batch summary to final summary\n",
    "        final_summary += batch_summary.content + \"\\n\\n\"\n",
    "        \n",
    "        # Reset summaries list for next batch\n",
    "        summaries = []\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
